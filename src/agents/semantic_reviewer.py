"""
è¯­ä¹‰ç†è§£ä¸“å®¶ (The Semantic Reviewer)
====================================

ä½¿ç”¨ RAG-Reviewer æ¶æ„è¿›è¡ŒéŸ³ä¹è¯­ä¹‰åˆ†æï¼š
- æè¿°ç¬¦åº“æ£€ç´¢
- CLaMP 3 éŸ³é¢‘ç¼–ç  (Native Implementation)
- è¯­ä¹‰æ ‡ç­¾æå–
"""

import time
import json
import torch
import torchaudio
import numpy as np
import requests
import sys
import os
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from tqdm import tqdm

from rich.console import Console
from rich.table import Table

# æ·»åŠ  lib è·¯å¾„ä»¥å¯¼å…¥ CLaMP 3 åŸç”Ÿæ¨¡å—
PROJECT_ROOT_DIR = Path(__file__).resolve().parent.parent.parent
LIB_DIR = PROJECT_ROOT_DIR / "lib" / "clamp3"
CODE_DIR = LIB_DIR / "code"
AUDIO_PREPROC_DIR = LIB_DIR / "preprocessing" / "audio"

sys.path.append(str(CODE_DIR))
sys.path.append(str(AUDIO_PREPROC_DIR))

# CLaMP 3 Native Imports
try:
    from config import (
        CLAMP3_WEIGHTS_PATH, 
        AUDIO_HIDDEN_SIZE, 
        AUDIO_NUM_LAYERS, 
        MAX_AUDIO_LENGTH,
        M3_HIDDEN_SIZE, 
        PATCH_NUM_LAYERS, 
        PATCH_LENGTH, 
        TEXT_MODEL_NAME, 
        CLAMP3_HIDDEN_SIZE, 
        CLAMP3_LOAD_M3,
        MAX_TEXT_LENGTH
    )
    from utils import CLaMP3Model
    from hf_pretrains import HuBERTFeature
    from transformers import BertConfig, AutoTokenizer
except ImportError as e:
    # é¦–æ¬¡è¿è¡Œæ—¶å¯èƒ½å› è·¯å¾„é—®é¢˜æŠ¥é”™ï¼Œæ­¤æ—¶æ— æ³•ç»§ç»­
    print(f"Error importing CLaMP 3 modules: {e}")
    # Define dummy variables to avoid linter errors before sys.path takes effect in runtime
    CLAMP3_WEIGHTS_PATH = ""

from src.config import config as app_config, PROJECT_ROOT, DESCRIPTOR_BANK_PATH
from src.schemas import AnalysisState, SemanticTags

console = Console()

# === é»˜è®¤æè¿°ç¬¦åº“ ===
DEFAULT_DESCRIPTORS = {
    "mood": [
        "Melancholic", "Euphoric", "Nostalgic", "Aggressive", "Peaceful",
        "Tense", "Hopeful", "Dark", "Uplifting", "Dreamy", "Energetic",
        "Sad", "Happy", "Anxious", "Calm", "Powerful", "Romantic"
    ],
    "genre": [
        "Pop", "Rock", "Jazz", "Classical", "Electronic", "Hip-Hop",
        "R&B", "Country", "Folk", "Metal", "Punk", "Blues", "Soul",
        "Funk", "Reggae", "Latin", "World", "Ambient", "Techno",
        "House", "Synthwave", "Vaporwave", "Lo-fi", "Trap"
    ],
    "instruments": [
        "Piano", "Guitar", "Drums", "Bass", "Violin", "Cello",
        "Synthesizer", "Trumpet", "Saxophone", "Flute", "Organ",
        "Strings", "Brass", "Woodwinds", "Percussion", "Vocals"
    ],
    "texture": [
        "Distorted", "Clean", "Reverb-heavy", "Dry", "Lo-fi",
        "Hi-fi", "Warm", "Cold", "Bright", "Dark", "Muddy",
        "Crisp", "Fuzzy", "Ethereal", "Gritty", "Smooth"
    ],
    "era": [
        "60s", "70s", "80s", "90s", "2000s", "2010s", "Modern",
        "Retro", "Vintage", "Contemporary", "Futuristic"
    ]
}


class DescriptorBank:
    """
    æè¿°ç¬¦åº“ - é™æ€èµ„äº§ä¸ Embedding ç¼“å­˜
    
    ç®¡ç†éŸ³ä¹æœ¯è¯­åŠå…¶å¯¹åº”çš„ CLaMP æ–‡æœ¬å‘é‡ã€‚
    """
    
    def __init__(self, bank_path: Optional[Path] = None):
        """
        åˆå§‹åŒ–æè¿°ç¬¦åº“
        
        Args:
            bank_path: æè¿°ç¬¦åº“ JSON æ–‡ä»¶è·¯å¾„
        """
        self.bank_path = bank_path or DESCRIPTOR_BANK_PATH
        self.cache_path = self.bank_path.parent / "descriptor_embeddings_clamp3.npy"
        
        self.descriptors: Dict[str, List[str]] = {}
        # å¹³é“ºçš„æ ‡ç­¾åˆ—è¡¨ï¼Œç”¨äºæ‰¹é‡è®¡ç®—
        self.flat_tags: List[Tuple[str, str]] = []  # (category, tag)
        self.embeddings: Optional[np.ndarray] = None # (N, D)
        
        self._loaded = False
        
    def load(self) -> None:
        """åŠ è½½æè¿°ç¬¦ JSON åŠå…¶ Embeddings (å¦‚æœå­˜åœ¨)"""
        if self._loaded:
            return
            
        # 1. åŠ è½½ JSON
        if self.bank_path.exists():
            console.print(f"[cyan]åŠ è½½æè¿°ç¬¦åº“: {self.bank_path}[/cyan]")
            with open(self.bank_path, 'r', encoding='utf-8') as f:
                self.descriptors = json.load(f)
        else:
            console.print("[yellow]ä½¿ç”¨é»˜è®¤æè¿°ç¬¦åº“[/yellow]")
            self.descriptors = DEFAULT_DESCRIPTORS
            # è‡ªåŠ¨ä¿å­˜é»˜è®¤åº“
            self.save_default()
            
        # 2. æ„å»ºå¹³é“ºåˆ—è¡¨
        self.flat_tags = []
        for category, tag_list in self.descriptors.items():
            for tag in tag_list:
                self.flat_tags.append((category, tag))
                
        # 3. å°è¯•åŠ è½½ç¼“å­˜çš„ Embeddings
        if self.cache_path.exists():
            try:
                self.embeddings = np.load(self.cache_path)
                console.print(f"[green]âœ“ å·²åŠ è½½ Embedding ç¼“å­˜: {self.embeddings.shape}[/green]")
                
                # æ ¡éªŒç¼“å­˜å¤§å°æ˜¯å¦åŒ¹é…
                if self.embeddings.shape[0] != len(self.flat_tags):
                    console.print("[yellow]âš  ç¼“å­˜å¤§å°ä¸åŒ¹é…ï¼Œå°†é‡æ–°è®¡ç®—[/yellow]")
                    self.embeddings = None
            except Exception as e:
                console.print(f"[red]åŠ è½½ç¼“å­˜å¤±è´¥: {e}[/red]")
                self.embeddings = None
        
        total = len(self.flat_tags)
        console.print(f"[green]âœ“ æè¿°ç¬¦å°±ç»ª: {total} ä¸ªæ ‡ç­¾[/green]")
        self._loaded = True
    
    def compute_missings(self, analyzer: 'SemanticAnalyzer') -> None:
        """
        è®¡ç®—ç¼ºå¤±çš„ Embeddings
        
        Args:
            analyzer: SemanticAnalyzer instance
        """
        if self.embeddings is not None:
            return
            
        console.print("[cyan]æ­£åœ¨è®¡ç®—æè¿°ç¬¦ Embeddings (CLaMP 3)...[/cyan]")
        
        texts = [f"A music track with {tag} {category}" for category, tag in self.flat_tags]
        
        # åˆ†æ‰¹å¤„ç†ä»¥é¿å… OOM
        batch_size = 32
        all_embeds = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i : i + batch_size]
            
            # ä½¿ç”¨ Analyzer çš„ encode_text æ–¹æ³•
            text_features = analyzer.encode_text_batch(batch_texts)
            if text_features is not None:
                all_embeds.append(text_features)
            else:
                raise RuntimeError("Failed to encode text batch")
                
        self.embeddings = np.vstack(all_embeds)
        
        # ä¿å­˜ç¼“å­˜
        np.save(self.cache_path, self.embeddings)
        console.print(f"[green]âœ“ Embeddings å·²è®¡ç®—å¹¶ä¿å­˜: {self.embeddings.shape}[/green]")

    def save_default(self) -> None:
        """ä¿å­˜é»˜è®¤æè¿°ç¬¦åº“åˆ°æ–‡ä»¶"""
        self.bank_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.bank_path, 'w', encoding='utf-8') as f:
            json.dump(DEFAULT_DESCRIPTORS, f, indent=2, ensure_ascii=False)


class SemanticAnalyzer:
    """
    è¯­ä¹‰åˆ†æå™¨ (Native CLaMP 3)
    
    ä½¿ç”¨åŸç”Ÿ CLaMP 3 å®ç°è¿›è¡ŒéŸ³é¢‘-æ–‡æœ¬è·¨æ¨¡æ€æ£€ç´¢ã€‚
    æµç¨‹ï¼šAudio -> MERT (m-a-p/MERT-v1-95M) -> CLaMP 3 Encoder -> Global Semantic Vector
    """
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.clamp_model = None
        self.mert_model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.descriptor_bank = DescriptorBank()
        self._loaded = False
        
        # MERT é…ç½®
        self.target_sr = 24000
        self.mert_model_name = "m-a-p/MERT-v1-95M"
        
    def _check_and_download_weights(self):
        """æ£€æŸ¥å¹¶ä¸‹è½½ CLaMP 3 æƒé‡"""
        # ä¿®æ­£è·¯å¾„ï¼šç¡®ä¿ä½¿ç”¨ config.py ä¸­å®šä¹‰çš„ç›¸å¯¹è·¯å¾„çš„ç»å¯¹ä½ç½®
        # config.py ä¸­çš„ CLAMP3_WEIGHTS_PATH æ˜¯ç›¸å¯¹è·¯å¾„ (e.g., "weights_clamp3_saas...")
        # æˆ‘ä»¬å‡è®¾å®ƒä½äº lib/clamp3/code ç›®å½•ä¸‹
        weights_path = CODE_DIR / CLAMP3_WEIGHTS_PATH
        
        if not weights_path.exists():
            console.print(f"[yellow]CLaMP 3 æƒé‡æœªæ‰¾åˆ°ï¼Œæ­£åœ¨ä¸‹è½½...[/yellow]")
            console.print(f"ç›®æ ‡è·¯å¾„: {weights_path}")
            
            url = "https://huggingface.co/sander-wood/clamp3/resolve/main/weights_clamp3_saas_h_size_768_t_model_FacebookAI_xlm-roberta-base_t_length_128_a_size_768_a_layers_12_a_length_128_s_size_768_s_layers_12_p_size_64_p_length_512.pth"
            
            try:
                response = requests.get(url, stream=True)
                response.raise_for_status()
                total_size = int(response.headers.get('content-length', 0))
                
                with open(weights_path, "wb") as f, tqdm(
                    desc="Downloading",
                    total=total_size,
                    unit="B",
                    unit_scale=True,
                    unit_divisor=1024,
                ) as bar:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            bar.update(len(chunk))
                console.print("[green]âœ“ CLaMP 3 æƒé‡ä¸‹è½½å®Œæˆ[/green]")
            except Exception as e:
                console.print(f"[red]ä¸‹è½½æƒé‡å¤±è´¥: {e}[/red]")
                raise e
        
        return weights_path

    def load_model(self) -> None:
        """åŠ è½½ CLaMP 3 å’Œ MERT æ¨¡å‹"""
        if self._loaded:
            return
            
        console.print(f"[cyan]æ­£åœ¨åŠ è½½è¯­ä¹‰åˆ†ææ¨¡å‹ (Device: {self.device})...[/cyan]")
        
        try:
            # 1. å‡†å¤‡æƒé‡
            weights_path = self._check_and_download_weights()
            
            # åˆå§‹åŒ– MERT æ¨¡å‹ (ç”¨äºéŸ³é¢‘ç‰¹å¾æå–)
            console.print(f"[yellow]Loading MERT model: {self.mert_model_name}...[/yellow]")
            # HuBERTFeature(pre_trained_folder, sample_rate, ...)
            self.mert_model = HuBERTFeature(self.mert_model_name, 24000)
            self.mert_model.to(self.device)
            self.mert_model.eval()
            console.print("[green]âœ“ MERT model loaded[/green]")
            
            # 3. åŠ è½½ CLaMP 3 (Semantic Encoder)
            console.print(f"[cyan]åŠ è½½ CLaMP 3 æ¨¡å‹...[/cyan]")
            
            # é…ç½®
            audio_config = BertConfig(
                vocab_size=1,
                hidden_size=AUDIO_HIDDEN_SIZE,
                num_hidden_layers=AUDIO_NUM_LAYERS,
                num_attention_heads=AUDIO_HIDDEN_SIZE//64,
                intermediate_size=AUDIO_HIDDEN_SIZE*4,
                max_position_embeddings=MAX_AUDIO_LENGTH
            )
            symbolic_config = BertConfig(
                vocab_size=1,
                hidden_size=M3_HIDDEN_SIZE,
                num_hidden_layers=PATCH_NUM_LAYERS,
                num_attention_heads=M3_HIDDEN_SIZE//64,
                intermediate_size=M3_HIDDEN_SIZE*4,
                max_position_embeddings=PATCH_LENGTH
            )
            
            # åˆå§‹åŒ–æ¨¡å‹
            self.clamp_model = CLaMP3Model(
                audio_config=audio_config,
                symbolic_config=symbolic_config,
                text_model_name=TEXT_MODEL_NAME,
                hidden_size=CLAMP3_HIDDEN_SIZE,
                load_m3=False # æ¨ç†æ—¶ä¸éœ€è¦åŠ è½½ M3 è®­ç»ƒæƒé‡ï¼Œåªéœ€è¦åŠ è½½ CLaMP3 æ•´ä½“æƒé‡
            )
            
            # åŠ è½½æƒé‡
            checkpoint = torch.load(weights_path, map_location="cpu", weights_only=True)
            self.clamp_model.load_state_dict(checkpoint['model'])
            console.print(f"Loaded CLaMP 3 Checkpoint from Epoch {checkpoint.get('epoch', '?')} with loss {checkpoint.get('min_eval_loss', '?')}")
            
            self.clamp_model.to(self.device)
            self.clamp_model.eval()
            
            # 4. åŠ è½½ Tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)
            
            # 5. åŠ è½½æè¿°ç¬¦åº“å¹¶è®¡ç®—ç¼ºå¤±çš„ Embeddings
            self._loaded = True
            self.descriptor_bank.load()
            self.descriptor_bank.compute_missings(self)
            
            console.print(f"[green]âœ“ æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆ[/green]")
            
        except Exception as e:
            console.print(f"[red]æ¨¡å‹åŠ è½½å¤±è´¥: {e}[/red]")
            import traceback
            console.print(traceback.format_exc())
            raise e

    def encode_text_batch(self, texts: List[str]) -> Optional[np.ndarray]:
        """
        æ‰¹é‡ç¼–ç æ–‡æœ¬
        """
        if not self._loaded:
            return None
            
        try:
            # Tokenize
            # ä½¿ç”¨ tokenizer.sep_token è¿æ¥ä¸éœ€è¦ï¼Œå› ä¸ºè¿™é‡Œæ˜¯ list of strings
            # ä½† extract_clamp3.py ä¸­å¤„ç†å•ä¸ª txt æ–‡ä»¶æ˜¯ join å tokenizeï¼Œè¿™é‡Œæˆ‘ä»¬å¤„ç† list
            # CLaMP 3 çš„ tokenizer æ˜¯ XLM-R
            
            # è¿™é‡Œçš„å¤„ç†é€»è¾‘å‚è€ƒ extract_clamp3.py ä¸­ .txt æ–‡ä»¶çš„å¤„ç†ï¼Œä½†å®ƒæ˜¯å¤„ç†ä¸€ä¸ªé•¿æ–‡æœ¬ã€‚
            # æˆ‘ä»¬æ˜¯å¤„ç† batch of short texts (tags).
            
            # å¯¹æ¯ä¸ªæ–‡æœ¬è¿›è¡Œ tokenize å’Œ padding
            encoded_input = self.tokenizer(
                texts, 
                padding=True, 
                truncation=True, 
                max_length=MAX_TEXT_LENGTH, 
                return_tensors="pt"
            )
            
            input_ids = encoded_input['input_ids'].to(self.device)
            attention_mask = encoded_input['attention_mask'].to(self.device)
            
            with torch.no_grad():
                text_features = self.clamp_model.get_text_features(
                    text_inputs=input_ids,
                    text_masks=attention_mask,
                    get_global=True
                )
                
                # å½’ä¸€åŒ–
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
            return text_features.cpu().numpy()
            
        except Exception as e:
            console.print(f"[red]æ–‡æœ¬ç¼–ç å¤±è´¥: {e}[/red]")
            return None

    def encode_audio(self, audio_path: Path) -> Optional[np.ndarray]:
        """
        è®¡ç®—éŸ³é¢‘çš„ CLaMP Embedding
        
        æµç¨‹:
        1. Load & Resample (24k)
        2. Split into 5s chunks
        3. MERT Feature Extraction -> Mean Pooling
        4. Concatenate chunks -> CLaMP 3 Audio Encoder -> Global Vector
        """
        if not self._loaded:
            return None
            
        try:
            # 1. åŠ è½½éŸ³é¢‘
            waveform, sr = torchaudio.load(str(audio_path))
            
            # è½¬æ¢ä¸ºå•å£°é“
            if waveform.shape[0] > 1:
                waveform = waveform.mean(dim=0, keepdim=True)
            
            # Resample to 24k for MERT
            if sr != self.target_sr:
                resampler = torchaudio.transforms.Resample(sr, self.target_sr)
                waveform = resampler(waveform)
                sr = self.target_sr
                
            # 2. åˆ‡åˆ† (5s windows)
            window_size = 5 # seconds
            window_samples = int(window_size * sr)
            
            # Pad if needed
            if waveform.shape[1] < window_samples:
                pad_len = window_samples - waveform.shape[1]
                waveform = torch.nn.functional.pad(waveform, (0, pad_len))
            
            # Split into chunks (no overlap as per extract_mert.py logic implied by sliding_window_overlap_in_percent=0.0 default)
            chunks = []
            for i in range(0, waveform.shape[1], window_samples):
                chunk = waveform[:, i:i+window_samples]
                if chunk.shape[1] < window_samples:
                    # Pad last chunk
                    pad_len = window_samples - chunk.shape[1]
                    chunk = torch.nn.functional.pad(chunk, (0, pad_len))
                chunks.append(chunk)
            
            if not chunks:
                return None
                
            chunks = torch.stack(chunks).to(self.device) # (N, 1, samples)
            
            # 3. MERT æå–
            mert_features_list = []
            with torch.no_grad():
                for i in range(chunks.shape[0]):
                    # HuBERTFeature expects (B, T) input, our chunks are (1, 1, samples) -> (1, samples)
                    wav_input = chunks[i] # (1, samples)
                    
                    # process_wav does padding/norm
                    wav_input = self.mert_model.process_wav(wav_input).to(self.device)
                    
                    # forward(input_values, layer=None, reduction="mean")
                    # layer=None means all layers, reduction="mean" means average over time
                    # But extract_mert.py uses layer=None, reduction="mean" -> returns [L, B, H]
                    # Wait, let's check extract_mert.py line 98: if mean_features: features = features.mean(dim=0, keepdim=True)
                    # extract_mert.py default reduction is 'mean'.
                    # In extract_mert.py:
                    # features = feature_extractor(wav_chunk, layer=layer, reduction=reduction)
                    # layer is None (default), reduction is 'mean' (default).
                    # HuBERTFeature.forward returns [L, B, H] if layer=None and reduction!="none" (actually reduction="mean" returns mean over time)
                    # Wait, HuBERTFeature code:
                    # if layer != None: ... else: out = torch.stack(out) # [L, B, T, H]
                    # if reduction == "mean": return out.mean(-2)
                    # So if layer=None, it returns [L, B, H].
                    
                    # CLaMP 3 expects MERT features. extract_clamp3.py loads .npy files.
                    # extract_mert.py saves features. If --mean_features is used (README says "averages across all layers and time steps"),
                    # README says "averages across all layers and time steps to produce a single feature per segment".
                    # extract_mert.py: if mean_features: features = features.mean(dim=0, keepdim=True)
                    # So we need to average over layers (dim 0).
                    
                    features = self.mert_model(wav_input, layer=None, reduction="mean") # [L, 1, H]
                    features = features.mean(dim=0, keepdim=True) # [1, 1, H]
                    mert_features_list.append(features)
            
            # Concatenate chunks -> (1, N_chunks, H) -> remove batch dim -> (N_chunks, H)
            # Actually CLaMP 3 expects (Batch, Seq, H) ?
            # extract_clamp3.py: input_data = np.load(filename) ... reshape(-1, input_data.size(-1))
            # It treats the whole file as a sequence of features.
            
            mert_features = torch.cat(mert_features_list, dim=0).squeeze(1) # (N_chunks, H)
            
            # Add zero vectors at start and end (from extract_clamp3.py line 122)
            zero_vec = torch.zeros((1, mert_features.size(-1)), device=self.device)
            mert_features = torch.cat((zero_vec, mert_features, zero_vec), 0)
            
            # 4. CLaMP 3 æ¨ç†
            # åˆ†æ®µå¤„ç† (MAX_AUDIO_LENGTH)
            # extract_clamp3.py Logic:
            input_data = mert_features
            max_input_length = MAX_AUDIO_LENGTH
            
            segment_list = []
            for i in range(0, len(input_data), max_input_length):
                segment_list.append(input_data[i:i+max_input_length])
            # Handle last segment special logic in extract_clamp3.py line 131: 
            # segment_list[-1] = input_data[-max_input_length:] 
            # (This seems to imply overlap for the last segment if it's short, or just taking the last N)
            if len(segment_list) > 0:
                segment_list[-1] = input_data[-max_input_length:]
            
            last_hidden_states_list = []
            
            with torch.no_grad():
                for input_segment in segment_list:
                    # Prepare masks
                    input_masks = torch.ones(input_segment.size(0), device=self.device)
                    
                    # Pad to MAX_AUDIO_LENGTH
                    pad_len = MAX_AUDIO_LENGTH - input_segment.size(0)
                    if pad_len > 0:
                        pad_indices = torch.zeros((pad_len, AUDIO_HIDDEN_SIZE), device=self.device)
                        input_segment = torch.cat((input_segment, pad_indices), 0)
                        
                        mask_pad = torch.zeros(pad_len, device=self.device)
                        input_masks = torch.cat((input_masks, mask_pad), 0)
                    
                    # CLaMP 3 Forward
                    last_hidden_states = self.clamp_model.get_audio_features(
                        audio_inputs=input_segment.unsqueeze(0), # (1, L, H)
                        audio_masks=input_masks.unsqueeze(0),    # (1, L)
                        get_global=True
                    )
                    last_hidden_states_list.append(last_hidden_states)
            
            # Aggregation (Weighted Average)
            # extract_clamp3.py line 166
            full_chunk_cnt = len(input_data) // max_input_length
            remain_chunk_len = len(input_data) % max_input_length
            
            if remain_chunk_len == 0:
                feature_weights = torch.tensor([max_input_length] * full_chunk_cnt, device=self.device).view(-1, 1)
            else:
                feature_weights = torch.tensor([max_input_length] * full_chunk_cnt + [remain_chunk_len], device=self.device).view(-1, 1)
            
            # Ensure dimensions match
            if len(last_hidden_states_list) != feature_weights.shape[0]:
                # Fallback or simple mean if logic mismatch
                feature_weights = torch.ones((len(last_hidden_states_list), 1), device=self.device)
            
            last_hidden_states_list = torch.concat(last_hidden_states_list, 0) # (N_seg, H)
            last_hidden_states_list = last_hidden_states_list * feature_weights
            final_feature = last_hidden_states_list.sum(dim=0) / feature_weights.sum() # (H,)
            
            # å½’ä¸€åŒ–
            final_feature = final_feature / final_feature.norm(dim=-1, keepdim=True)
            
            return final_feature.unsqueeze(0).cpu().numpy() # (1, D)
            
        except Exception as e:
            console.print(f"[red]éŸ³é¢‘ç¼–ç å¤±è´¥: {audio_path} - {e}[/red]")
            import traceback
            console.print(traceback.format_exc())
            return None
        
    def retrieve_tags(
        self,
        audio_path: Path,
        top_k: int = 5
    ) -> SemanticTags:
        """
        æ£€ç´¢æœ€åŒ¹é…çš„è¯­ä¹‰æ ‡ç­¾
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            top_k: æ¯ä¸ªç±»åˆ«è¿”å›çš„æ ‡ç­¾æ•°é‡
            
        Returns:
            SemanticTags å¯¹è±¡
        """
        if not self._loaded:
            self.load_model()
            
        console.print(f"\n[cyan]è¯­ä¹‰æ£€ç´¢: {audio_path.name}[/cyan]")
        
        # è·å–éŸ³é¢‘åµŒå…¥
        # Shape: (1, D)
        audio_embedding = self.encode_audio(audio_path)
        
        tags = SemanticTags()
        
        if audio_embedding is not None and self.descriptor_bank.embeddings is not None:
            # æ–‡æœ¬ Embeddings: (N, D)
            text_embeddings = self.descriptor_bank.embeddings
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦: (1, D) @ (D, N) -> (1, N)
            # å‡è®¾éƒ½å·²å½’ä¸€åŒ–
            similarities = (audio_embedding @ text_embeddings.T).squeeze() # (N,)
            
            # æ•´ç†ç»“æœ
            # ä¸ºäº†æŒ‰ç±»åˆ«ç­›é€‰ï¼Œæˆ‘ä»¬éœ€è¦éå†æ‰€æœ‰ç»“æœ
            # å»ºç«‹ä¸€ä¸ª (score, category, tag) çš„åˆ—è¡¨
            results = []
            for idx, score in enumerate(similarities):
                cat, tag = self.descriptor_bank.flat_tags[idx]
                results.append({
                    "category": cat,
                    "tag": tag,
                    "score": float(score)
                })
            
            # æŒ‰ç±»åˆ«åˆ†ç»„å¹¶æ’åº
            from collections import defaultdict
            grouped_results = defaultdict(list)
            for res in results:
                grouped_results[res["category"]].append(res)
                
            # å¡«å…… SemanticTags
            # æ¯ä¸ªç±»åˆ«å– Top-K
            all_scores = {}
            
            for category, items in grouped_results.items():
                # é™åºæ’åº
                items.sort(key=lambda x: x["score"], reverse=True)
                
                # å– Top-K (ä¸”åˆ†æ•°éœ€å¤§äºæŸä¸ªå¾®å°é˜ˆå€¼ï¼Œæ¯”å¦‚ 0.05)
                top_items = [item for item in items[:top_k] if item["score"] > 0.05]
                
                tag_names = [item["tag"] for item in top_items]
                
                # èµ‹å€¼ç»™ SemanticTags å¯¹åº”çš„å­—æ®µ
                if category == "mood":
                    tags.mood = tag_names
                elif category == "genre":
                    tags.genre = tag_names
                elif category == "instruments":
                    tags.instruments = tag_names
                elif category == "texture":
                    tags.texture = tag_names
                
                # è®°å½•ç½®ä¿¡åº¦
                for item in top_items:
                    all_scores[item["tag"]] = item["score"]
            
            tags.confidence_scores = all_scores
            
        return tags
        
    def analyze(
        self,
        audio_path: Path,
        stems_paths: Optional[Dict] = None
    ) -> SemanticTags:
        """
        å®Œæ•´çš„è¯­ä¹‰åˆ†ææµç¨‹
        """
        console.print("\n[bold cyan]ğŸ­ è¯­ä¹‰åˆ†æ (CLaMP 3)[/bold cyan]")
        
        # åˆ†æåŸå§‹éŸ³é¢‘
        main_tags = self.retrieve_tags(audio_path)
        
        return main_tags


def analyze_semantics(state: AnalysisState) -> AnalysisState:
    """
    LangGraph èŠ‚ç‚¹å‡½æ•°ï¼šè¯­ä¹‰åˆ†æ
    """
    console.print("\n[bold magenta]=== è¯­ä¹‰è¯„å®¡ä¸“å®¶ ===[/bold magenta]")
    
    start_time = time.time()
    
    try:
        analyzer = SemanticAnalyzer()
        
        # å†³å®šåˆ†æä»€ä¹ˆï¼šåŸæ›²
        tags = analyzer.analyze(state['audio_path'])
        
        # æ›´æ–°çŠ¶æ€
        new_state = state.copy()
        new_state['semantic_tags'] = tags
        
        if 'processing_time' not in new_state:
            new_state['processing_time'] = {}
        new_state['processing_time']["semantic_analysis"] = time.time() - start_time
        
        return new_state
        
    except Exception as e:
        console.print(f"[red][ERROR] è¯­ä¹‰åˆ†æå¤±è´¥: {e}[/red]")
        import traceback
        console.print(traceback.format_exc())
        
        new_state = state.copy()
        if 'errors' not in new_state:
            new_state['errors'] = []
        new_state['errors'].append(f"è¯­ä¹‰åˆ†æå¤±è´¥: {str(e)}")
        return new_state


# === åˆå§‹åŒ–æè¿°ç¬¦åº“ ===
def init_descriptor_bank():
    """åˆå§‹åŒ–å¹¶ä¿å­˜é»˜è®¤æè¿°ç¬¦åº“"""
    bank = DescriptorBank()
    bank.save_default()


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python -m src.agents.semantic_reviewer <audio_file>")
        sys.exit(1)
        
    audio_file = Path(sys.argv[1])
    
    if not audio_file.exists():
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {audio_file}")
        sys.exit(1)
        
    analyzer = SemanticAnalyzer()
    tags = analyzer.analyze(audio_file)
    
    print("\nTop Tags:")
    print(tags.model_dump_json(indent=2))
