"""
ç¬¦å·è½¬å½•ä¸“å®¶ (The Transcriber)
==============================

ä½¿ç”¨ Basic Pitch å°†åˆ†ç¦»åçš„éŸ³è½¨è½¬æ¢ä¸º MIDI æ•°æ®ã€‚
"""

import time
import json
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import math
from collections import Counter

import numpy as np
import librosa
from rich.console import Console
from rich.table import Table

from src.config import config, OUTPUT_DIR
from src.schemas import (
    StemType, 
    AnalysisState, 
    MIDIData, 
    NoteEvent,
    MusicalFeatures,
    StemAnalysis,
    ChordInfo
)

console = Console()


def calculate_md5(file_path: Path) -> str:
    """è®¡ç®—æ–‡ä»¶ MD5"""
    try:
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception as e:
        return f"Error: {e}"


class AudioTranscriber:
    """
    Basic Pitch ç¬¦å·è½¬å½•å™¨
    
    å°†åˆ†è½¨éŸ³é¢‘è½¬å½•ä¸º MIDI æ•°æ®ï¼Œæå–ï¼š
    - éŸ³ç¬¦äº‹ä»¶ (Note On/Off, Velocity)
    - BPM æ£€æµ‹
    - è°ƒæ€§åˆ†æ
    """
    
    def __init__(self):
        """åˆå§‹åŒ–è½¬å½•å™¨"""
        self.model = None
        self._loaded = False
        
    def load_model(self) -> None:
        """åŠ è½½ Basic Pitch æ¨¡å‹"""
        if self._loaded:
            return
            
        console.print("[cyan]æ­£åœ¨åŠ è½½ Basic Pitch æ¨¡å‹...[/cyan]")
        
        try:
            from basic_pitch.inference import predict
            from basic_pitch import ICASSP_2022_MODEL_PATH
            
            self._predict = predict
            self._model_path = ICASSP_2022_MODEL_PATH
            self._loaded = True
            console.print("[green]âœ“ Basic Pitch æ¨¡å‹åŠ è½½å®Œæˆ[/green]")
            
        except ImportError:
            console.print("[yellow]âš  Basic Pitch æœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼[/yellow]")
            self._loaded = True
            self._predict = None
            
    def transcribe_stem(
        self,
        stem_path: Path,
        stem_type: StemType,
        output_dir: Optional[Path] = None
    ) -> MIDIData:
        """
        è½¬å½•å•ä¸ªåˆ†è½¨
        
        Args:
            stem_path: åˆ†è½¨éŸ³é¢‘è·¯å¾„
            stem_type: åˆ†è½¨ç±»å‹
            output_dir: MIDI è¾“å‡ºç›®å½•
            
        Returns:
            MIDIData å¯¹è±¡
        """
        if not self._loaded:
            self.load_model()
            
        md5 = calculate_md5(stem_path)
        console.print(f"  è½¬å½• {stem_type.value} [dim]({md5[:8]})[/dim]...", end=" ")
        
        notes: List[NoteEvent] = []
        detected_tempo = None
        
        if self._predict is not None and stem_path.exists():
            try:
                # ä½¿ç”¨ Basic Pitch è¿›è¡Œè½¬å½•
                model_output, midi_data, note_events = self._predict(
                    str(stem_path),
                    onset_threshold=config.basic_pitch.onset_threshold,
                    frame_threshold=config.basic_pitch.frame_threshold,
                    minimum_note_length=config.basic_pitch.minimum_note_length,
                    minimum_frequency=config.basic_pitch.minimum_frequency,
                    maximum_frequency=config.basic_pitch.maximum_frequency,
                )
                
                # è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ•°æ®ç»“æ„
                for note in note_events:
                    notes.append(NoteEvent(
                        pitch=int(note[2]),  # MIDI pitch
                        start_time=float(note[0]),
                        end_time=float(note[1]),
                        velocity=int(note[3] * 127) if len(note) > 3 else 100
                    ))
                    
                # ä¿å­˜ MIDI æ–‡ä»¶
                if output_dir and midi_data:
                    output_dir.mkdir(parents=True, exist_ok=True)
                    midi_path = output_dir / f"{stem_type.value}.mid"
                    midi_data.write(str(midi_path))
                    
                console.print(f"[green]âœ“[/green] ({len(notes)} éŸ³ç¬¦)")
                
            except Exception as e:
                console.print(f"[red]âœ— é”™è¯¯: {e}[/red]")
        else:
            # æ¨¡æ‹Ÿæ¨¡å¼ï¼šç”Ÿæˆç¤ºä¾‹æ•°æ®
            console.print("[yellow]âš  æ¨¡æ‹Ÿæ¨¡å¼[/yellow]")
            
        return MIDIData(
            stem_type=stem_type,
            notes=notes,
            tempo=detected_tempo
        )
        
    def transcribe_all_stems(
        self,
        stems_paths: Dict[StemType, Path],
        output_dir: Optional[Path] = None
    ) -> Dict[StemType, MIDIData]:
        """
        è½¬å½•æ‰€æœ‰åˆ†è½¨
        
        é’ˆå¯¹æ€§è½¬å½•ç­–ç•¥ï¼š
        - Drums: ä»…æå–èŠ‚å¥ä¿¡æ¯
        - Bass: ä»…æå–ä½éŸ³çº¿
        - Vocals: æå–æ—‹å¾‹çº¿
        - Other: æå–å’Œå£°ä¿¡æ¯
        """
        console.print("\n[bold cyan]ğŸ¼ å¼€å§‹ç¬¦å·è½¬å½•[/bold cyan]")
        
        results: Dict[StemType, MIDIData] = {}
        
        for stem_type, stem_path in stems_paths.items():
            midi_data = self.transcribe_stem(stem_path, stem_type, output_dir)
            results[stem_type] = midi_data
            
        return results
        
    def analyze_features(
        self, 
        midi_data: Dict[StemType, MIDIData],
        stems_paths: Optional[Dict[StemType, Path]] = None
    ) -> MusicalFeatures:
        """
        ä» MIDI æ•°æ®ä¸­åˆ†æéŸ³ä¹ç‰¹å¾
        
        Returns:
            MusicalFeatures åŒ…å« BPMã€è°ƒæ€§ç­‰ä¿¡æ¯
        """
        console.print("\n[cyan]åˆ†æéŸ³ä¹ç‰¹å¾...[/cyan]")
        
        # 1. æ”¶é›†æ‰€æœ‰éŸ³ç¬¦ç”¨äºå…¨å±€åˆ†æ
        all_notes: List[NoteEvent] = []
        stem_analyses: Dict[str, StemAnalysis] = {}
        
        for stem_type, data in midi_data.items():
            all_notes.extend(data.notes)
            
            # åˆ†è½¨è¯¦ç»†ç»Ÿè®¡
            analysis = self._analyze_stem(stem_type, data.notes)
            stem_analyses[stem_type.value] = analysis
            console.print(f"  - {stem_type.value}: {analysis.description}")

        # ä¼°ç®—æ€»æ—¶é•¿
        duration_seconds = 0.0
        if all_notes:
            duration_seconds = max(n.end_time for n in all_notes)

        # 2. æ£€æµ‹ BPM (ä¼˜å…ˆå°è¯•éŸ³é¢‘æ£€æµ‹)
        bpm = None
        if stems_paths:
            # ä¼˜å…ˆä½¿ç”¨ Drums, å…¶æ¬¡ Bass, å†æ¬¡ Mix (å¦‚æœä¸åœ¨è¿™é‡Œçš„è¯)
            target_stem = stems_paths.get(StemType.DRUMS) or stems_paths.get(StemType.BASS)
            if target_stem and target_stem.exists():
                bpm = self._detect_bpm_from_audio(target_stem)
                
        if bpm is None:
            bpm = self._detect_bpm(all_notes)
        
        console.print(f"  - æœ€ç»ˆ BPM: {bpm:.1f}")

        # 3. æ£€æµ‹è°ƒæ€§ (ä¼˜å…ˆå°è¯•éŸ³é¢‘æ£€æµ‹)
        key = None
        if stems_paths:
             # ä¼˜å…ˆä½¿ç”¨å’Œå£°ä¸°å¯Œçš„è½¨é“
             target_stem = stems_paths.get(StemType.PIANO) or stems_paths.get(StemType.GUITAR) or stems_paths.get(StemType.OTHER)
             if target_stem and target_stem.exists():
                 key = self._detect_key_from_audio(target_stem)
                 
        if key is None:
            key = self._detect_key(all_notes)
            
        console.print(f"  - æœ€ç»ˆè°ƒæ€§: {key}")

        # 4. æ¨æ–­å’Œå¼¦è¿›è¡Œ
        chord_progression = self._infer_chords(all_notes, bpm, duration_seconds)
        if chord_progression:
            prog_str = " -> ".join([c.chord_name for c in chord_progression[:4]])
            console.print(f"  - å’Œå¼¦è¿›è¡Œ: {prog_str} ...")

        return MusicalFeatures(
            bpm=bpm,
            key=key,
            time_signature="4/4", # æš‚æ—¶å‡å®š 4/4
            chord_progression=chord_progression,
            duration_seconds=duration_seconds,
            stem_analyses=stem_analyses
        )

    def _analyze_stem(self, stem_type: StemType, notes: List[NoteEvent]) -> StemAnalysis:
        """åˆ†æå•ä¸ªåˆ†è½¨çš„ç»Ÿè®¡ç‰¹å¾"""
        if not notes:
            return StemAnalysis(
                stem_type=stem_type.value,
                note_count=0,
                note_density=0.0,
                pitch_range=(0, 0),
                average_velocity=0.0,
                active_ratio=0.0,
                description="æ— éŸ³ç¬¦æ•°æ®"
            )

        # åŸºç¡€ç»Ÿè®¡
        count = len(notes)
        duration = max(n.end_time for n in notes) if notes else 1.0
        pitches = [n.pitch for n in notes]
        velocities = [n.velocity for n in notes]
        
        min_pitch, max_pitch = min(pitches), max(pitches)
        avg_velocity = sum(velocities) / count
        density = count / duration if duration > 0 else 0
        
        # æ´»è·ƒåº¦è®¡ç®— (ç®€å•çš„æ€»æ—¶å€¼/æ€»æ—¶é•¿)
        total_note_duration = sum(n.end_time - n.start_time for n in notes)
        active_ratio = min(1.0, total_note_duration / duration) if duration > 0 else 0

        # ç”Ÿæˆæè¿°
        desc_parts = []
        
        # å¯†åº¦æè¿°
        if density > 8: desc_parts.append("æå…¶å¯†é›†")
        elif density > 4: desc_parts.append("æ¼”å¥æ´»è·ƒ")
        elif density < 0.5: desc_parts.append("é›¶æ˜Ÿç‚¹ç¼€")
        else: desc_parts.append("èŠ‚å¥é€‚ä¸­")
        
        # éŸ³åŸŸæè¿°
        range_semitones = max_pitch - min_pitch
        if range_semitones > 24: desc_parts.append("éŸ³åŸŸè·¨åº¦æå¤§")
        elif range_semitones < 7: desc_parts.append("éŸ³åŸŸé›†ä¸­")
        
        # åŠ›åº¦æè¿°
        if avg_velocity > 100: desc_parts.append("åŠ›åº¦å¼ºåŠ²")
        elif avg_velocity < 60: desc_parts.append("è§¦é”®è½»æŸ”")

        description = f"{'ï¼Œ'.join(desc_parts)} (èŒƒå›´: {self._midi_to_note_name(min_pitch)}-{self._midi_to_note_name(max_pitch)})"

        return StemAnalysis(
            stem_type=stem_type.value,
            note_count=count,
            note_density=round(density, 2),
            pitch_range=(min_pitch, max_pitch),
            average_velocity=round(avg_velocity, 1),
            active_ratio=round(active_ratio, 2),
            description=description
        )

    def _detect_bpm_from_audio(self, audio_path: Path) -> Optional[float]:
        """ä»éŸ³é¢‘ä¸­ç›´æ¥æ£€æµ‹ BPM"""
        try:
            console.print(f"  [dim]éŸ³é¢‘ BPM åˆ†æ: {audio_path.name}...[/dim]", end=" ")
            # ä»…è¯»å–å‰ 60 ç§’ä»¥æé«˜é€Ÿåº¦
            y, sr = librosa.load(str(audio_path), sr=22050, duration=60)
            if len(y) == 0:
                return None
            
            onset_env = librosa.onset.onset_strength(y=y, sr=sr)
            tempo, _ = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)
            
            if np.ndim(tempo) > 0:
                tempo = tempo[0]
                
            console.print(f"[green]{tempo:.1f}[/green]")
            return float(tempo)
        except Exception as e:
            console.print(f"[red]å¤±è´¥: {e}[/red]")
            return None

    def _detect_key_from_audio(self, audio_path: Path) -> Optional[str]:
        """ä»éŸ³é¢‘ä¸­ç›´æ¥æ£€æµ‹è°ƒæ€§"""
        try:
            console.print(f"  [dim]éŸ³é¢‘è°ƒæ€§åˆ†æ: {audio_path.name}...[/dim]", end=" ")
            y, sr = librosa.load(str(audio_path), sr=22050, duration=60)
            if len(y) == 0:
                return None
                
            # è®¡ç®— Chroma CQT
            chroma = librosa.feature.chroma_cqt(y=y, sr=sr)
            chroma_sum = np.sum(chroma, axis=1)
            
            # K-S Profiles
            major_profile = np.array([6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88])
            minor_profile = np.array([6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17])
            
            chroma_norm = chroma_sum / (np.sum(chroma_sum) + 1e-6) * 100
            
            pitch_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
            best_corr = -1.0
            best_key = "Unknown"
            
            for i in range(12):
                # Major
                c_maj = np.corrcoef(np.roll(chroma_norm, -i), major_profile)[0, 1]
                if c_maj > best_corr:
                    best_corr = c_maj
                    best_key = f"{pitch_names[i]} Major"
                    
                # Minor
                c_min = np.corrcoef(np.roll(chroma_norm, -i), minor_profile)[0, 1]
                if c_min > best_corr:
                    best_corr = c_min
                    best_key = f"{pitch_names[i]} Minor"
            
            console.print(f"[green]{best_key}[/green]")
            return best_key
        except Exception as e:
            console.print(f"[red]å¤±è´¥: {e}[/red]")
            return None

    def _detect_bpm(self, notes: List[NoteEvent]) -> float:
        """åŸºäº IOI ç›´æ–¹å›¾ä¼°ç®— BPM"""
        if len(notes) < 10:
            return 120.0 # é»˜è®¤å€¼
            
        # æå–æ‰€æœ‰ onset
        onsets = sorted([n.start_time for n in notes])
        
        # è®¡ç®—ç›¸é‚» IOI (Inter-Onset Intervals)
        iois = []
        for i in range(len(onsets) - 1):
            diff = onsets[i+1] - onsets[i]
            if 0.1 < diff < 2.0: # è¿‡æ»¤æ‰æçŸ­(trill)å’Œæé•¿(pause)çš„é—´éš”
                iois.append(diff)
                
        if not iois:
            return 120.0
            
        # ç›´æ–¹å›¾ç»Ÿè®¡
        bins = np.arange(0.1, 2.0, 0.02) # 20ms bin
        hist, bin_edges = np.histogram(iois, bins=bins)
        
        # æ‰¾åˆ°å³°å€¼
        peak_idx = np.argmax(hist)
        peak_ioi = (bin_edges[peak_idx] + bin_edges[peak_idx+1]) / 2
        
        bpm = 60.0 / peak_ioi
        
        # å°† BPM å½’ä¸€åŒ–åˆ° 70-160 ä¹‹é—´
        while bpm < 70: bpm *= 2
        while bpm > 160: bpm /= 2
            
        return round(bpm, 1)

    def _detect_key(self, notes: List[NoteEvent]) -> str:
        """ä½¿ç”¨ Krumhansl-Schmuckler ç®—æ³•ä¼°ç®—è°ƒæ€§"""
        if not notes:
            return "Unknown"
            
        # K-S Profiles
        major_profile = [6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88]
        minor_profile = [6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17]
        
        # è®¡ç®— Pitch Class åˆ†å¸ƒ (æŒ‰æ—¶é•¿åŠ æƒ)
        pc_distribution = [0.0] * 12
        for n in notes:
            pc = n.pitch % 12
            duration = n.end_time - n.start_time
            pc_distribution[pc] += duration
            
        # å½’ä¸€åŒ–
        total_duration = sum(pc_distribution)
        if total_duration == 0: return "Unknown"
        pc_distribution = [x / total_duration * 100 for x in pc_distribution] # ç¼©æ”¾åˆ°ç™¾åˆ†æ¯”ä»¥ä¾¿ä¸ profile åŒ¹é…
        
        # è®¡ç®—ç›¸å…³æ€§
        best_corr = -1.0
        best_key = "Unknown"
        
        pitch_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
        
        # éå† 12 ä¸ªåŠéŸ³ç§»ä½
        for i in range(12):
            # æµ‹è¯• Major
            corr_major = np.corrcoef(
                np.roll(pc_distribution, -i), 
                major_profile
            )[0, 1]
            
            if corr_major > best_corr:
                best_corr = corr_major
                best_key = f"{pitch_names[i]} Major"
                
            # æµ‹è¯• Minor
            corr_minor = np.corrcoef(
                np.roll(pc_distribution, -i), 
                minor_profile
            )[0, 1]
            
            if corr_minor > best_corr:
                best_corr = corr_minor
                best_key = f"{pitch_names[i]} Minor"
                
        return best_key

    def _infer_chords(self, notes: List[NoteEvent], bpm: float, duration: float) -> List[ChordInfo]:
        """ç®€å•çš„å’Œå¼¦æ¨æ–­"""
        if not notes or bpm <= 0:
            return []
            
        seconds_per_beat = 60.0 / bpm
        seconds_per_measure = seconds_per_beat * 4 # å‡è®¾ 4/4
        
        chords = []
        num_measures = int(math.ceil(duration / seconds_per_measure))
        
        for m in range(num_measures):
            start = m * seconds_per_measure
            end = start + seconds_per_measure
            
            # è·å–è¯¥å°èŠ‚å†…çš„éŸ³ç¬¦
            measure_notes = [n for n in notes if start <= n.start_time < end]
            if not measure_notes:
                continue
                
            # ç»Ÿè®¡ Pitch Class
            pcs = [n.pitch % 12 for n in measure_notes]
            if not pcs: continue
            
            pc_counts = Counter(pcs)
            root_pc = pc_counts.most_common(1)[0][0]
            
            # ç®€å•åˆ¤æ–­å¤§/å°ä¸‰å’Œå¼¦
            has_major_3rd = (root_pc + 4) % 12 in pcs
            has_minor_3rd = (root_pc + 3) % 12 in pcs
            
            chord_name = self._midi_to_note_name(root_pc, with_octave=False)
            if has_minor_3rd:
                chord_name += "m"
            elif not has_major_3rd:
                # æ—¢æ— å¤§ä¸‰ä¹Ÿæ— å°ä¸‰ï¼Œå¯èƒ½æ˜¯æŒ‚ç•™æˆ–ä»…ä»…æ˜¯å•éŸ³ï¼Œæš‚ä¸”åªæ ‡æ ¹éŸ³
                pass
                
            chords.append(ChordInfo(
                chord_name=chord_name,
                start_time=round(start, 2),
                end_time=round(end, 2)
            ))
            
        return chords

    def _midi_to_note_name(self, midi: int, with_octave: bool = True) -> str:
        """MIDI éŸ³é«˜è½¬éŸ³å"""
        names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
        pitch = midi % 12
        octave = midi // 12 - 1
        name = names[pitch]
        return f"{name}{octave}" if with_octave else name


def transcribe_stems(state: AnalysisState) -> AnalysisState:
    """
    LangGraph èŠ‚ç‚¹å‡½æ•°ï¼šç¬¦å·è½¬å½•
    
    ä¾èµ–äºåˆ†ç¦»èŠ‚ç‚¹å®Œæˆåè¿è¡Œã€‚
    æ›´æ–°çŠ¶æ€ä¸­çš„ midi_data å’Œ musical_features å­—æ®µã€‚
    """
    console.print("\n[bold magenta]â•â•â• ç¬¦å·è½¬å½•ä¸“å®¶ â•â•â•[/bold magenta]")
    
    if not state.separation_complete:
        console.print("[yellow]âš  åˆ†ç¦»å°šæœªå®Œæˆï¼Œè·³è¿‡è½¬å½•[/yellow]")
        return state
        
    start_time = time.time()
    
    try:
        transcriber = AudioTranscriber()
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = OUTPUT_DIR / state.audio_path.stem / "midi"
        
        # è½¬å½•æ‰€æœ‰åˆ†è½¨
        midi_results = transcriber.transcribe_all_stems(
            state.stems_paths,
            output_dir
        )
        
        # åˆ†æéŸ³ä¹ç‰¹å¾
        features = transcriber.analyze_features(midi_results, state.stems_paths)
        
        # æ›´æ–°çŠ¶æ€
        state.midi_data = midi_results
        state.musical_features = features
        state.transcription_complete = True
        state.processing_time["transcription"] = time.time() - start_time
        
        # æ‰“å°æ‘˜è¦
        table = Table(title="è½¬å½•ç»“æœæ‘˜è¦")
        table.add_column("åˆ†è½¨", style="cyan")
        table.add_column("éŸ³ç¬¦æ•°", justify="right")
        
        for stem_type, midi in midi_results.items():
            table.add_row(stem_type.value, str(len(midi.notes)))
            
        console.print(table)
        
    except Exception as e:
        state.errors.append(f"è½¬å½•å¤±è´¥: {str(e)}")
        console.print(f"[red]âœ— è½¬å½•å¤±è´¥: {e}[/red]")
        
    return state


# === å‘½ä»¤è¡Œæµ‹è¯•å…¥å£ ===
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python transcriber.py <stem_file>")
        sys.exit(1)
        
    stem_file = Path(sys.argv[1])
    transcriber = AudioTranscriber()
    
    try:
        result = transcriber.transcribe_stem(stem_file, StemType.VOCALS)
        console.print(f"\n[bold green]è½¬å½•å®Œæˆ:[/bold green]")
        console.print(f"  éŸ³ç¬¦æ•°: {len(result.notes)}")
        if result.notes:
            console.print(f"  é¦–ä¸ªéŸ³ç¬¦: pitch={result.notes[0].pitch}, "
                         f"start={result.notes[0].start_time:.2f}s")
    except Exception as e:
        console.print(f"[red]é”™è¯¯: {e}[/red]")
        sys.exit(1)
